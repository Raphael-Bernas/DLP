{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "1PFIiBzYeJB-"
      },
      "source": [
        "# TP 3  : Graph Neural Networks Architecture\n",
        "\n",
        "**Théo Rudkiewicz, Cyriaque Rousselot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "bZ7im39QeJCD"
      },
      "source": [
        "# TUTORIAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "HgY1VvuheJCE"
      },
      "source": [
        "### Install Pytorch Geometric\n",
        "\n",
        "To handle graph data, we use the library Pytorch Geometric : https://pytorch-geometric.readthedocs.io/en/latest/\n",
        "\n",
        "*   If you use _Google Colab_, simply run the following cell to install Pytorch Geometric (**advised**).\n",
        "*   If you plan using your _own environment_, follow the documentation to install Pytorch Geometric : https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html and skip the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBLVniLWeJCF",
        "outputId": "4a9df563-b241-4f5f-9032-badb6f5b9bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_scatter-2.1.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu124\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_sparse-0.6.18%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu124\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_cluster-1.6.3%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt25cu124\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt25cu124\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "########## INSTALL TORCH GEOMETRIC ##################\n",
        "# https://pytorch-geometric.readthedocs.io/en/latest/\n",
        "#####################################################\n",
        "import torch\n",
        "\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "    return version.split(\"+\")[0]\n",
        "\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "\n",
        "def format_cuda_version(version):\n",
        "    return \"cu\" + version.replace(\".\", \"\")\n",
        "\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "71TDRRYheJCI"
      },
      "source": [
        "### Import required packages\n",
        "\n",
        "Run the following cell to import all required packages. This cell **must not** be modified.\n",
        "\n",
        "To significantly accelerate your training, it is advised to use GPU. Using Google Colab, you need to activate it :\n",
        "\n",
        "*   Edit --> Notebook Setting --> Hardware accelerator --> GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "hpUMh9GzeJCI"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "################## PACKAGES #########################\n",
        "#####################################################\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_geometric.nn as graphnn\n",
        "from sklearn.metrics import f1_score\n",
        "from torch_geometric.datasets import PPI\n",
        "from torch_geometric.loader import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "YzJnGvSjeJCJ"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We use the Protein-Protein Interaction (PPI) network dataset which includes:\n",
        "- 20 graphs for training\n",
        "- 2 graphs for validation\n",
        "- 2 graphs for testing\n",
        "\n",
        "One graph of the PPI dataset has on average 2372 nodes. Each node has:\n",
        "- 50 features : positional gene sets / motif gene / immunological signatures ...\n",
        "- 121 (binary) labels : gene ontology sets (way to classify gene products like proteins).\n",
        "\n",
        "**This problem aims to predict, for a given PPI graph, the correct nodes' labels**.\n",
        "\n",
        "**It is a node (multi-label) classification task** (trained using supervised learning, with labels to be predicted for each node).\n",
        "\n",
        "For your curiosity, more detailed information on the dataset and some applications:\n",
        "- https://cs.stanford.edu/~jure/pubs/pathways-psb18.pdf\n",
        "- https://arxiv.org/abs/1707.04638\n",
        "\n",
        "To understand how a graph data is implemented in Pytorch Geometric, refer to : https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptY_TXaDeJCK",
        "outputId": "5edbdf1b-f609-40f6-fdb5-dda5738af3a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in the train dataset:  20\n",
            "Number of samples in the val dataset:  2\n",
            "Number of samples in the test dataset:  2\n",
            "Output of one sample from the train dataset:  Data(x=[1767, 50], edge_index=[2, 32318], y=[1767, 121])\n",
            "Edge_index :\n",
            "tensor([[   0,    0,    0,  ..., 1744, 1745, 1749],\n",
            "        [ 372, 1101,  766,  ..., 1745, 1744, 1739]])\n",
            "Number of features per node:  50\n",
            "Number of classes per node:  121\n"
          ]
        }
      ],
      "source": [
        "### LOAD DATASETS\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# Train Dataset\n",
        "train_dataset = PPI(root=\"\", split=\"train\")\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "# Val Dataset\n",
        "val_dataset = PPI(root=\"\", split=\"val\")\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "# Test Dataset\n",
        "test_dataset = PPI(root=\"\", split=\"test\")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Number of features and classes\n",
        "n_features, n_classes = train_dataset[0].x.shape[1], train_dataset[0].y.shape[1]\n",
        "\n",
        "print(\"Number of samples in the train dataset: \", len(train_dataset))\n",
        "print(\"Number of samples in the val dataset: \", len(test_dataset))\n",
        "print(\"Number of samples in the test dataset: \", len(test_dataset))\n",
        "print(\"Output of one sample from the train dataset: \", train_dataset[0])\n",
        "print(\"Edge_index :\")\n",
        "print(train_dataset[0].edge_index)\n",
        "print(\"Number of features per node: \", n_features)\n",
        "print(\"Number of classes per node: \", n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "jlnAadEDeJCK"
      },
      "source": [
        "### Define a basic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "CnXu0yXBeJCL"
      },
      "source": [
        "Here we define a very simple Graph Neural Network model which will be used as our baseline. This model consists of three graph convolutional layers (from https://arxiv.org/pdf/1609.02907.pdf). The first two layers computes 256 features, followed by an ELU activation function. The last layer is used for (multi-label) classification task, computing 121 features (for each node)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "IOSt2JgKeJCM"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "################## MODEL ############################\n",
        "#####################################################\n",
        "class BasicGraphModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.graphconv1 = graphnn.GCNConv(input_size, hidden_size)\n",
        "        self.graphconv2 = graphnn.GCNConv(hidden_size, hidden_size)\n",
        "        self.graphconv3 = graphnn.GCNConv(hidden_size, output_size)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.graphconv1(x, edge_index)\n",
        "        x = self.elu(x)\n",
        "        x = self.graphconv2(x, edge_index)\n",
        "        x = self.elu(x)\n",
        "        x = self.graphconv3(x, edge_index)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "B8YbSmyFeJCM"
      },
      "source": [
        "Next function is designed to evaluate the performance of the model, computing the F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "DBRBNrABeJCM"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "############### TEST FUNCTION #######################\n",
        "#####################################################\n",
        "def evaluate(model, loss_fcn, device, dataloader):\n",
        "    score_list_batch = []\n",
        "\n",
        "    model.eval()\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch.x, batch.edge_index)\n",
        "        loss_test = loss_fcn(output, batch.y)\n",
        "        predict = np.where(output.detach().cpu().numpy() >= 0, 1, 0)\n",
        "        score = f1_score(batch.y.cpu().numpy(), predict, average=\"micro\")\n",
        "        score_list_batch.append(score)\n",
        "\n",
        "    return np.array(score_list_batch).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "CDPqXwFteJCN"
      },
      "source": [
        "Next we construct the function to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "zmPNX8kweJCN"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "############## TRAIN FUNCTION #######################\n",
        "#####################################################\n",
        "def train(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader):\n",
        "    epoch_list = []\n",
        "    scores_list = []\n",
        "\n",
        "    # loop over epochs\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        # loop over batches\n",
        "        for i, train_batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            train_batch_device = train_batch.to(device)\n",
        "            # logits is the output of the model\n",
        "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
        "            # compute the loss\n",
        "            loss = loss_fcn(logits, train_batch_device.y)\n",
        "            # optimizer step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        loss_data = np.array(losses).mean()\n",
        "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            # evaluate the model on the validation set\n",
        "            # computes the f1-score (see next function)\n",
        "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
        "            print(\"F1-Score: {:.4f}\".format(score))\n",
        "            scores_list.append(score)\n",
        "            epoch_list.append(epoch)\n",
        "\n",
        "    return epoch_list, scores_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "hRLZRyrveJCN"
      },
      "source": [
        "Let's train this model !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "O3oSDygYeJCN",
        "outputId": "1b724752-a7a7-4cc1-b574-6aa59d14052f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Device:  cuda\n",
            "Epoch 00001 | Loss: 0.6368\n",
            "F1-Score: 0.4550\n",
            "Epoch 00002 | Loss: 0.5804\n",
            "Epoch 00003 | Loss: 0.5618\n",
            "Epoch 00004 | Loss: 0.5549\n",
            "Epoch 00005 | Loss: 0.5506\n",
            "Epoch 00006 | Loss: 0.5463\n",
            "F1-Score: 0.5102\n",
            "Epoch 00007 | Loss: 0.5424\n",
            "Epoch 00008 | Loss: 0.5382\n",
            "Epoch 00009 | Loss: 0.5344\n",
            "Epoch 00010 | Loss: 0.5315\n",
            "Epoch 00011 | Loss: 0.5290\n",
            "F1-Score: 0.5229\n",
            "Epoch 00012 | Loss: 0.5265\n",
            "Epoch 00013 | Loss: 0.5240\n",
            "Epoch 00014 | Loss: 0.5217\n",
            "Epoch 00015 | Loss: 0.5194\n",
            "Epoch 00016 | Loss: 0.5172\n",
            "F1-Score: 0.5314\n",
            "Epoch 00017 | Loss: 0.5151\n",
            "Epoch 00018 | Loss: 0.5130\n",
            "Epoch 00019 | Loss: 0.5109\n",
            "Epoch 00020 | Loss: 0.5087\n",
            "Epoch 00021 | Loss: 0.5064\n",
            "F1-Score: 0.5137\n",
            "Epoch 00022 | Loss: 0.5038\n",
            "Epoch 00023 | Loss: 0.5015\n",
            "Epoch 00024 | Loss: 0.4995\n",
            "Epoch 00025 | Loss: 0.4974\n",
            "Epoch 00026 | Loss: 0.4955\n",
            "F1-Score: 0.5419\n",
            "Epoch 00027 | Loss: 0.4936\n",
            "Epoch 00028 | Loss: 0.4918\n",
            "Epoch 00029 | Loss: 0.4900\n",
            "Epoch 00030 | Loss: 0.4882\n",
            "Epoch 00031 | Loss: 0.4865\n",
            "F1-Score: 0.5552\n",
            "Epoch 00032 | Loss: 0.4848\n",
            "Epoch 00033 | Loss: 0.4833\n",
            "Epoch 00034 | Loss: 0.4819\n",
            "Epoch 00035 | Loss: 0.4807\n",
            "Epoch 00036 | Loss: 0.4797\n",
            "F1-Score: 0.5654\n",
            "Epoch 00037 | Loss: 0.4791\n",
            "Epoch 00038 | Loss: 0.4788\n",
            "Epoch 00039 | Loss: 0.4769\n",
            "Epoch 00040 | Loss: 0.4760\n",
            "Epoch 00041 | Loss: 0.4742\n",
            "F1-Score: 0.5594\n",
            "Epoch 00042 | Loss: 0.4725\n",
            "Epoch 00043 | Loss: 0.4707\n",
            "Epoch 00044 | Loss: 0.4693\n",
            "Epoch 00045 | Loss: 0.4677\n",
            "Epoch 00046 | Loss: 0.4663\n",
            "F1-Score: 0.5642\n",
            "Epoch 00047 | Loss: 0.4652\n",
            "Epoch 00048 | Loss: 0.4635\n",
            "Epoch 00049 | Loss: 0.4619\n",
            "Epoch 00050 | Loss: 0.4605\n",
            "Epoch 00051 | Loss: 0.4597\n",
            "F1-Score: 0.5699\n",
            "Epoch 00052 | Loss: 0.4589\n",
            "Epoch 00053 | Loss: 0.4579\n",
            "Epoch 00054 | Loss: 0.4568\n",
            "Epoch 00055 | Loss: 0.4562\n",
            "Epoch 00056 | Loss: 0.4551\n",
            "F1-Score: 0.5818\n",
            "Epoch 00057 | Loss: 0.4538\n",
            "Epoch 00058 | Loss: 0.4527\n",
            "Epoch 00059 | Loss: 0.4520\n",
            "Epoch 00060 | Loss: 0.4511\n",
            "Epoch 00061 | Loss: 0.4503\n",
            "F1-Score: 0.5722\n",
            "Epoch 00062 | Loss: 0.4495\n",
            "Epoch 00063 | Loss: 0.4491\n",
            "Epoch 00064 | Loss: 0.4491\n",
            "Epoch 00065 | Loss: 0.4499\n",
            "Epoch 00066 | Loss: 0.4498\n",
            "F1-Score: 0.5849\n",
            "Epoch 00067 | Loss: 0.4494\n",
            "Epoch 00068 | Loss: 0.4505\n",
            "Epoch 00069 | Loss: 0.4493\n",
            "Epoch 00070 | Loss: 0.4474\n",
            "Epoch 00071 | Loss: 0.4457\n",
            "F1-Score: 0.5799\n",
            "Epoch 00072 | Loss: 0.4452\n",
            "Epoch 00073 | Loss: 0.4453\n",
            "Epoch 00074 | Loss: 0.4435\n",
            "Epoch 00075 | Loss: 0.4427\n",
            "Epoch 00076 | Loss: 0.4411\n",
            "F1-Score: 0.5866\n",
            "Epoch 00077 | Loss: 0.4401\n",
            "Epoch 00078 | Loss: 0.4398\n",
            "Epoch 00079 | Loss: 0.4392\n",
            "Epoch 00080 | Loss: 0.4375\n",
            "Epoch 00081 | Loss: 0.4369\n",
            "F1-Score: 0.5995\n",
            "Epoch 00082 | Loss: 0.4364\n",
            "Epoch 00083 | Loss: 0.4361\n",
            "Epoch 00084 | Loss: 0.4351\n",
            "Epoch 00085 | Loss: 0.4346\n",
            "Epoch 00086 | Loss: 0.4342\n",
            "F1-Score: 0.5883\n",
            "Epoch 00087 | Loss: 0.4342\n",
            "Epoch 00088 | Loss: 0.4354\n",
            "Epoch 00089 | Loss: 0.4364\n",
            "Epoch 00090 | Loss: 0.4353\n",
            "Epoch 00091 | Loss: 0.4341\n",
            "F1-Score: 0.5964\n",
            "Epoch 00092 | Loss: 0.4337\n",
            "Epoch 00093 | Loss: 0.4341\n",
            "Epoch 00094 | Loss: 0.4329\n",
            "Epoch 00095 | Loss: 0.4322\n",
            "Epoch 00096 | Loss: 0.4324\n",
            "F1-Score: 0.6010\n",
            "Epoch 00097 | Loss: 0.4321\n",
            "Epoch 00098 | Loss: 0.4302\n",
            "Epoch 00099 | Loss: 0.4291\n",
            "Epoch 00100 | Loss: 0.4274\n",
            "Epoch 00101 | Loss: 0.4261\n",
            "F1-Score: 0.6025\n",
            "Epoch 00102 | Loss: 0.4259\n",
            "Epoch 00103 | Loss: 0.4251\n",
            "Epoch 00104 | Loss: 0.4247\n",
            "Epoch 00105 | Loss: 0.4238\n",
            "Epoch 00106 | Loss: 0.4250\n",
            "F1-Score: 0.5941\n",
            "Epoch 00107 | Loss: 0.4240\n",
            "Epoch 00108 | Loss: 0.4235\n",
            "Epoch 00109 | Loss: 0.4244\n",
            "Epoch 00110 | Loss: 0.4240\n",
            "Epoch 00111 | Loss: 0.4232\n",
            "F1-Score: 0.6112\n",
            "Epoch 00112 | Loss: 0.4235\n",
            "Epoch 00113 | Loss: 0.4248\n",
            "Epoch 00114 | Loss: 0.4242\n",
            "Epoch 00115 | Loss: 0.4257\n",
            "Epoch 00116 | Loss: 0.4257\n",
            "F1-Score: 0.5821\n",
            "Epoch 00117 | Loss: 0.4257\n",
            "Epoch 00118 | Loss: 0.4229\n",
            "Epoch 00119 | Loss: 0.4218\n",
            "Epoch 00120 | Loss: 0.4204\n",
            "Epoch 00121 | Loss: 0.4211\n",
            "F1-Score: 0.6009\n",
            "Epoch 00122 | Loss: 0.4209\n",
            "Epoch 00123 | Loss: 0.4189\n",
            "Epoch 00124 | Loss: 0.4185\n",
            "Epoch 00125 | Loss: 0.4192\n",
            "Epoch 00126 | Loss: 0.4184\n",
            "F1-Score: 0.5959\n",
            "Epoch 00127 | Loss: 0.4191\n",
            "Epoch 00128 | Loss: 0.4192\n",
            "Epoch 00129 | Loss: 0.4207\n",
            "Epoch 00130 | Loss: 0.4213\n",
            "Epoch 00131 | Loss: 0.4208\n",
            "F1-Score: 0.6051\n",
            "Epoch 00132 | Loss: 0.4196\n",
            "Epoch 00133 | Loss: 0.4192\n",
            "Epoch 00134 | Loss: 0.4177\n",
            "Epoch 00135 | Loss: 0.4185\n",
            "Epoch 00136 | Loss: 0.4218\n",
            "F1-Score: 0.6148\n",
            "Epoch 00137 | Loss: 0.4204\n",
            "Epoch 00138 | Loss: 0.4227\n",
            "Epoch 00139 | Loss: 0.4214\n",
            "Epoch 00140 | Loss: 0.4230\n",
            "Epoch 00141 | Loss: 0.4234\n",
            "F1-Score: 0.6431\n",
            "Epoch 00142 | Loss: 0.4233\n",
            "Epoch 00143 | Loss: 0.4254\n",
            "Epoch 00144 | Loss: 0.4222\n",
            "Epoch 00145 | Loss: 0.4238\n",
            "Epoch 00146 | Loss: 0.4259\n",
            "F1-Score: 0.5717\n",
            "Epoch 00147 | Loss: 0.4215\n",
            "Epoch 00148 | Loss: 0.4190\n",
            "Epoch 00149 | Loss: 0.4198\n",
            "Epoch 00150 | Loss: 0.4214\n",
            "Epoch 00151 | Loss: 0.4179\n",
            "F1-Score: 0.6373\n",
            "Epoch 00152 | Loss: 0.4186\n",
            "Epoch 00153 | Loss: 0.4192\n",
            "Epoch 00154 | Loss: 0.4180\n",
            "Epoch 00155 | Loss: 0.4172\n",
            "Epoch 00156 | Loss: 0.4172\n",
            "F1-Score: 0.6389\n",
            "Epoch 00157 | Loss: 0.4153\n",
            "Epoch 00158 | Loss: 0.4135\n",
            "Epoch 00159 | Loss: 0.4154\n",
            "Epoch 00160 | Loss: 0.4128\n",
            "Epoch 00161 | Loss: 0.4131\n",
            "F1-Score: 0.6360\n",
            "Epoch 00162 | Loss: 0.4103\n",
            "Epoch 00163 | Loss: 0.4089\n",
            "Epoch 00164 | Loss: 0.4065\n",
            "Epoch 00165 | Loss: 0.4054\n",
            "Epoch 00166 | Loss: 0.4047\n",
            "F1-Score: 0.6431\n",
            "Epoch 00167 | Loss: 0.4034\n",
            "Epoch 00168 | Loss: 0.4027\n",
            "Epoch 00169 | Loss: 0.4022\n",
            "Epoch 00170 | Loss: 0.4017\n",
            "Epoch 00171 | Loss: 0.4014\n",
            "F1-Score: 0.6399\n",
            "Epoch 00172 | Loss: 0.4009\n",
            "Epoch 00173 | Loss: 0.4016\n",
            "Epoch 00174 | Loss: 0.4021\n",
            "Epoch 00175 | Loss: 0.4023\n",
            "Epoch 00176 | Loss: 0.4032\n",
            "F1-Score: 0.6305\n",
            "Epoch 00177 | Loss: 0.4047\n",
            "Epoch 00178 | Loss: 0.4049\n",
            "Epoch 00179 | Loss: 0.4066\n",
            "Epoch 00180 | Loss: 0.4049\n",
            "Epoch 00181 | Loss: 0.4034\n",
            "F1-Score: 0.6250\n",
            "Epoch 00182 | Loss: 0.4036\n",
            "Epoch 00183 | Loss: 0.4020\n",
            "Epoch 00184 | Loss: 0.4015\n",
            "Epoch 00185 | Loss: 0.4012\n",
            "Epoch 00186 | Loss: 0.4021\n",
            "F1-Score: 0.6125\n",
            "Epoch 00187 | Loss: 0.4026\n",
            "Epoch 00188 | Loss: 0.4041\n",
            "Epoch 00189 | Loss: 0.4068\n",
            "Epoch 00190 | Loss: 0.4108\n",
            "Epoch 00191 | Loss: 0.4159\n",
            "F1-Score: 0.6549\n",
            "Epoch 00192 | Loss: 0.4148\n",
            "Epoch 00193 | Loss: 0.4165\n",
            "Epoch 00194 | Loss: 0.4163\n",
            "Epoch 00195 | Loss: 0.4186\n",
            "Epoch 00196 | Loss: 0.4182\n",
            "F1-Score: 0.5914\n",
            "Epoch 00197 | Loss: 0.4117\n",
            "Epoch 00198 | Loss: 0.4094\n",
            "Epoch 00199 | Loss: 0.4097\n",
            "Epoch 00200 | Loss: 0.4103\n"
          ]
        }
      ],
      "source": [
        "### DEVICE GPU OR CPU : will select GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"\\nDevice: \", device)\n",
        "\n",
        "### Max number of epochs\n",
        "max_epochs = 200\n",
        "\n",
        "### DEFINE THE MODEL\n",
        "basic_model = BasicGraphModel(input_size=n_features, hidden_size=256, output_size=n_classes).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "### DEFINE LOSS FUNCTION\n",
        "loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "### DEFINE OPTIMIZER\n",
        "optimizer = torch.optim.Adam(basic_model.parameters(), lr=0.005)\n",
        "\n",
        "### TRAIN THE MODEL\n",
        "epoch_list, basic_model_scores = train(\n",
        "    basic_model,\n",
        "    loss_fcn,\n",
        "    device,\n",
        "    optimizer,\n",
        "    max_epochs,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "bhzHztVKeJCO"
      },
      "source": [
        "Let's evaluate the performance of this basic model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "2i_gokoteJCO",
        "outputId": "d1ad703c-9e61-4959-eed0-575f22ac13c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic Model : F1-Score on the validation set: 0.6578\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### F1-SCORE ON VALID DATASET\n",
        "score_valid = evaluate(basic_model, loss_fcn, device, val_dataloader)\n",
        "print(\"Basic Model : F1-Score on the validation set: {:.4f}\".format(score_valid))\n",
        "\n",
        "\n",
        "### PLOT EVOLUTION OF F1-SCORE W.R.T EPOCHS\n",
        "def plot_f1_score(epoch_list, scores):\n",
        "    plt.figure(figsize=[10, 5])\n",
        "    plt.plot(epoch_list, scores)\n",
        "    plt.title(\"Evolution of F1-Score w.r.t epochs\")\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_f1_score(epoch_list, basic_model_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "j-eCAND6eJCO"
      },
      "source": [
        "# QUESTIONS\n",
        "\n",
        "## Grading\n",
        "\n",
        "You will be graded on 5 questions. You will need to provide at least 4 files :\n",
        "1. This Notebook\n",
        "2. `class_model_gnn.py`\n",
        "3. `model.pth` (the file **must be of size less than 50Mo** but 20Mo should be enough to get a very good model)\n",
        "4. `conv_as_message_passing.py`\n",
        "\n",
        "If the function you defined passes all the tests, you will get the full grade. Otherwise we  will look at the intermediate questions in the notebook to give you partial credit.\n",
        "\n",
        "\n",
        "\n",
        " Please provide clear, short and __bold font__ answers.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "QS2h-H9geJCO"
      },
      "source": [
        "> Question 1 : Design, build and train a model with a F1-score higher than 93% on validation set (**HINT :** https://arxiv.org/pdf/1710.10903.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "6cYwN6xZeJCP"
      },
      "source": [
        " Provide two files : (https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
        " -  a file  `class_model_gnn.py` containing the class inheriting from `torch.nn.Module` architecture of your final model to load\n",
        " -  a `model.pth` file : the model weights\n",
        "\n",
        " We will  test your model on final F1-Score on a test set. You must not use the test set for hyperparameter training.\n",
        "\n",
        "Intermediate question :\n",
        "\n",
        " Provide the script for training, and a plot of the training loss.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "L6EmYK5veJCP",
        "outputId": "9f042f41-8441-4ac9-9f8f-d29a22842a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Define model ( in your class_model_gnn.py)\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "model = StudentModel()\n",
        "\n",
        "## Save the model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "\n",
        "### This is the part we will run in the inference to grade your model\n",
        "## Load the model\n",
        "model = StudentModel()  # !  Important : No argument\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "zwlI67mxlL7e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "\n",
        "# Define model\n",
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, sigma=nn.ELU()):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.W = nn.Linear(in_dim, out_dim)\n",
        "        self.a = nn.Linear(2 * out_dim, 1)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        N = x.size(0)\n",
        "        h = self.W(x)  # N x out_dim\n",
        "\n",
        "        # Initialize attention output with very negative values\n",
        "        attn_output = torch.full((N, N), float('-inf')).to(x.device)\n",
        "\n",
        "        # Compute attention only for edges in edge_index\n",
        "        for i, j in edge_index.t():\n",
        "            attn_input = torch.cat([h[i], h[j]], dim=0)  # 2*out_dim\n",
        "            attn_output[i, j] = self.a(attn_input).squeeze(0)\n",
        "\n",
        "        attn_output = self.leaky_relu(attn_output)\n",
        "        attn_output = self.softmax(attn_output)\n",
        "\n",
        "        # Update node embedding\n",
        "        h_prime = torch.matmul(attn_output, h)\n",
        "        h_prime = self.sigma(h_prime)\n",
        "\n",
        "        return h_prime\n",
        "\n",
        "class MultiHeadGAT(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads=1, concat=True):\n",
        "        super(MultiHeadGAT, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.concat = concat\n",
        "        self.attention_heads = nn.ModuleList([GATLayer(in_dim, hidden_dim) for _ in range(num_heads)])\n",
        "        self.out_layer = GATLayer(hidden_dim * num_heads if concat else hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        head_outputs = [head(x, adj) for head in self.attention_heads]\n",
        "\n",
        "        if self.concat:\n",
        "            x = torch.cat(head_outputs, dim=1)\n",
        "        else:\n",
        "            x = torch.stack(head_outputs, dim=0).mean(dim=0)\n",
        "\n",
        "        x = self.out_layer(x, adj)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MyGAT(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads=1, num_inter_layers=1, concat=True, implemented = False, dropout=0.):\n",
        "        super(MyGAT, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        if implemented:\n",
        "            Head_GAT_in = MultiHeadGAT(in_dim, hidden_dim, hidden_dim, num_heads, concat)\n",
        "            Head_GAT_inter = MultiHeadGAT(hidden_dim * num_heads if concat else hidden_dim, hidden_dim, hidden_dim, num_heads, concat)\n",
        "            Head_GAT_out = nn.Linear(hidden_dim * num_heads if concat else hidden_dim, out_dim)\n",
        "        else:\n",
        "            Head_GAT_in = GATConv(in_dim, hidden_dim, heads=num_heads, concat=concat, dropout=dropout)\n",
        "            Head_GAT_inter = GATConv(hidden_dim * num_heads if concat else hidden_dim, hidden_dim, heads=num_heads, concat=concat, dropout=dropout)\n",
        "            Head_GAT_out = nn.Linear(hidden_dim * num_heads if concat else hidden_dim, out_dim)\n",
        "\n",
        "        # Add initial multi-head GAT layer\n",
        "        self.layers.append(Head_GAT_in)\n",
        "\n",
        "        # Add intermediate multi-head GAT layers\n",
        "        for _ in range(num_inter_layers):\n",
        "            self.layers.append(Head_GAT_inter)\n",
        "\n",
        "        # Add final averaging GAT layer\n",
        "        self.layers.append(Head_GAT_out)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        n = len(self.layers)\n",
        "        for i in range(n-1):\n",
        "            x = self.layers[i](x, adj)\n",
        "            x = self.leaky_relu(x)\n",
        "        x = self.layers[-1](x).flatten(1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S6t1BXi1eJCP",
        "outputId": "c5dfc76e-f88a-46ec-b198-d067cc440d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Device:  cuda\n",
            "Epoch 00001 | Loss: 0.8730\n",
            "F1-Score: 0.4536\n",
            "Epoch 00002 | Loss: 0.5625\n",
            "Epoch 00003 | Loss: 0.5531\n",
            "Epoch 00004 | Loss: 0.5446\n",
            "Epoch 00005 | Loss: 0.5333\n",
            "Epoch 00006 | Loss: 0.5289\n",
            "F1-Score: 0.4820\n",
            "Epoch 00007 | Loss: 0.5252\n",
            "Epoch 00008 | Loss: 0.5227\n",
            "Epoch 00009 | Loss: 0.5174\n",
            "Epoch 00010 | Loss: 0.5081\n",
            "Epoch 00011 | Loss: 0.5012\n",
            "F1-Score: 0.5037\n",
            "Epoch 00012 | Loss: 0.5003\n",
            "Epoch 00013 | Loss: 0.5010\n",
            "Epoch 00014 | Loss: 0.4990\n",
            "Epoch 00015 | Loss: 0.4918\n",
            "Epoch 00016 | Loss: 0.4839\n",
            "F1-Score: 0.5283\n",
            "Epoch 00017 | Loss: 0.4808\n",
            "Epoch 00018 | Loss: 0.4756\n",
            "Epoch 00019 | Loss: 0.4690\n",
            "Epoch 00020 | Loss: 0.4676\n",
            "Epoch 00021 | Loss: 0.4650\n",
            "F1-Score: 0.5756\n",
            "Epoch 00022 | Loss: 0.4603\n",
            "Epoch 00023 | Loss: 0.4511\n",
            "Epoch 00024 | Loss: 0.4450\n",
            "Epoch 00025 | Loss: 0.4611\n",
            "Epoch 00026 | Loss: 0.4575\n",
            "F1-Score: 0.5675\n",
            "Epoch 00027 | Loss: 0.4503\n",
            "Epoch 00028 | Loss: 0.4382\n",
            "Epoch 00029 | Loss: 0.4291\n",
            "Epoch 00030 | Loss: 0.4211\n",
            "Epoch 00031 | Loss: 0.4165\n",
            "F1-Score: 0.6084\n",
            "Epoch 00032 | Loss: 0.4139\n",
            "Epoch 00033 | Loss: 0.4039\n",
            "Epoch 00034 | Loss: 0.3946\n",
            "Epoch 00035 | Loss: 0.3861\n",
            "Epoch 00036 | Loss: 0.3895\n",
            "F1-Score: 0.6559\n",
            "Epoch 00037 | Loss: 0.3858\n",
            "Epoch 00038 | Loss: 0.3809\n",
            "Epoch 00039 | Loss: 0.3761\n",
            "Epoch 00040 | Loss: 0.3644\n",
            "Epoch 00041 | Loss: 0.3540\n",
            "F1-Score: 0.7020\n",
            "Epoch 00042 | Loss: 0.3452\n",
            "Epoch 00043 | Loss: 0.3385\n",
            "Epoch 00044 | Loss: 0.3401\n",
            "Epoch 00045 | Loss: 0.3420\n",
            "Epoch 00046 | Loss: 0.3519\n",
            "F1-Score: 0.7032\n",
            "Epoch 00047 | Loss: 0.3488\n",
            "Epoch 00048 | Loss: 0.3406\n",
            "Epoch 00049 | Loss: 0.3317\n",
            "Epoch 00050 | Loss: 0.3194\n",
            "Epoch 00051 | Loss: 0.3092\n",
            "F1-Score: 0.7450\n",
            "Epoch 00052 | Loss: 0.3017\n",
            "Epoch 00053 | Loss: 0.2981\n",
            "Epoch 00054 | Loss: 0.2969\n",
            "Epoch 00055 | Loss: 0.2924\n",
            "Epoch 00056 | Loss: 0.2819\n",
            "F1-Score: 0.7662\n",
            "Epoch 00057 | Loss: 0.2718\n",
            "Epoch 00058 | Loss: 0.2722\n",
            "Epoch 00059 | Loss: 0.2660\n",
            "Epoch 00060 | Loss: 0.2635\n",
            "Epoch 00061 | Loss: 0.2615\n",
            "F1-Score: 0.7832\n",
            "Epoch 00062 | Loss: 0.2607\n",
            "Epoch 00063 | Loss: 0.2547\n",
            "Epoch 00064 | Loss: 0.2500\n",
            "Epoch 00065 | Loss: 0.2458\n",
            "Epoch 00066 | Loss: 0.2438\n",
            "F1-Score: 0.7847\n",
            "Epoch 00067 | Loss: 0.2413\n",
            "Epoch 00068 | Loss: 0.2410\n",
            "Epoch 00069 | Loss: 0.2331\n",
            "Epoch 00070 | Loss: 0.2268\n",
            "Epoch 00071 | Loss: 0.2260\n",
            "F1-Score: 0.8105\n",
            "Epoch 00072 | Loss: 0.2386\n",
            "Epoch 00073 | Loss: 0.2291\n",
            "Epoch 00074 | Loss: 0.2293\n",
            "Epoch 00075 | Loss: 0.2246\n",
            "Epoch 00076 | Loss: 0.2134\n",
            "F1-Score: 0.8068\n",
            "Epoch 00077 | Loss: 0.2085\n",
            "Epoch 00078 | Loss: 0.2041\n",
            "Epoch 00079 | Loss: 0.2022\n",
            "Epoch 00080 | Loss: 0.2009\n",
            "Epoch 00081 | Loss: 0.1989\n",
            "F1-Score: 0.8303\n",
            "Epoch 00082 | Loss: 0.1959\n",
            "Epoch 00083 | Loss: 0.1942\n",
            "Epoch 00084 | Loss: 0.1940\n",
            "Epoch 00085 | Loss: 0.1879\n",
            "Epoch 00086 | Loss: 0.1830\n",
            "F1-Score: 0.8393\n",
            "Epoch 00087 | Loss: 0.1806\n",
            "Epoch 00088 | Loss: 0.1824\n",
            "Epoch 00089 | Loss: 0.1846\n",
            "Epoch 00090 | Loss: 0.1769\n",
            "Epoch 00091 | Loss: 0.1774\n",
            "F1-Score: 0.8457\n",
            "Epoch 00092 | Loss: 0.1772\n",
            "Epoch 00093 | Loss: 0.1825\n",
            "Epoch 00094 | Loss: 0.1799\n",
            "Epoch 00095 | Loss: 0.1678\n",
            "Epoch 00096 | Loss: 0.1586\n",
            "F1-Score: 0.8534\n",
            "Epoch 00097 | Loss: 0.1601\n",
            "Epoch 00098 | Loss: 0.1567\n",
            "Epoch 00099 | Loss: 0.1545\n",
            "Epoch 00100 | Loss: 0.1572\n",
            "Epoch 00101 | Loss: 0.1569\n",
            "F1-Score: 0.8556\n",
            "Epoch 00102 | Loss: 0.1625\n",
            "Epoch 00103 | Loss: 0.1585\n",
            "Epoch 00104 | Loss: 0.1515\n",
            "Epoch 00105 | Loss: 0.1450\n",
            "Epoch 00106 | Loss: 0.1435\n",
            "F1-Score: 0.8636\n",
            "Epoch 00107 | Loss: 0.1441\n",
            "Epoch 00108 | Loss: 0.1440\n",
            "Epoch 00109 | Loss: 0.1435\n",
            "Epoch 00110 | Loss: 0.1390\n",
            "Epoch 00111 | Loss: 0.1344\n",
            "F1-Score: 0.8795\n",
            "Epoch 00112 | Loss: 0.1277\n",
            "Epoch 00113 | Loss: 0.1236\n",
            "Epoch 00114 | Loss: 0.1240\n",
            "Epoch 00115 | Loss: 0.1258\n",
            "Epoch 00116 | Loss: 0.1254\n",
            "F1-Score: 0.8800\n",
            "Epoch 00117 | Loss: 0.1260\n",
            "Epoch 00118 | Loss: 0.1292\n",
            "Epoch 00119 | Loss: 0.1253\n",
            "Epoch 00120 | Loss: 0.1221\n",
            "Epoch 00121 | Loss: 0.1195\n",
            "F1-Score: 0.8839\n",
            "Epoch 00122 | Loss: 0.1169\n",
            "Epoch 00123 | Loss: 0.1150\n",
            "Epoch 00124 | Loss: 0.1116\n",
            "Epoch 00125 | Loss: 0.1075\n",
            "Epoch 00126 | Loss: 0.1047\n",
            "F1-Score: 0.8951\n",
            "Epoch 00127 | Loss: 0.1065\n",
            "Epoch 00128 | Loss: 0.1176\n",
            "Epoch 00129 | Loss: 0.1214\n",
            "Epoch 00130 | Loss: 0.1271\n",
            "Epoch 00131 | Loss: 0.1255\n",
            "F1-Score: 0.8880\n",
            "Epoch 00132 | Loss: 0.1167\n",
            "Epoch 00133 | Loss: 0.1155\n",
            "Epoch 00134 | Loss: 0.1184\n",
            "Epoch 00135 | Loss: 0.1100\n",
            "Epoch 00136 | Loss: 0.1047\n",
            "F1-Score: 0.8963\n",
            "Epoch 00137 | Loss: 0.0978\n",
            "Epoch 00138 | Loss: 0.0924\n",
            "Epoch 00139 | Loss: 0.0873\n",
            "Epoch 00140 | Loss: 0.0835\n",
            "Epoch 00141 | Loss: 0.0840\n",
            "F1-Score: 0.9065\n",
            "Epoch 00142 | Loss: 0.0848\n",
            "Epoch 00143 | Loss: 0.0876\n",
            "Epoch 00144 | Loss: 0.0891\n",
            "Epoch 00145 | Loss: 0.0946\n",
            "Epoch 00146 | Loss: 0.0951\n",
            "F1-Score: 0.9033\n",
            "Epoch 00147 | Loss: 0.0939\n",
            "Epoch 00148 | Loss: 0.0901\n",
            "Epoch 00149 | Loss: 0.0905\n",
            "Epoch 00150 | Loss: 0.0895\n",
            "Epoch 00151 | Loss: 0.0865\n",
            "F1-Score: 0.9086\n",
            "Epoch 00152 | Loss: 0.0899\n",
            "Epoch 00153 | Loss: 0.0938\n",
            "Epoch 00154 | Loss: 0.0893\n",
            "Epoch 00155 | Loss: 0.0861\n",
            "Epoch 00156 | Loss: 0.0856\n",
            "F1-Score: 0.9084\n",
            "Epoch 00157 | Loss: 0.0864\n",
            "Epoch 00158 | Loss: 0.0815\n",
            "Epoch 00159 | Loss: 0.0867\n",
            "Epoch 00160 | Loss: 0.0869\n",
            "Epoch 00161 | Loss: 0.0886\n",
            "F1-Score: 0.9030\n",
            "Epoch 00162 | Loss: 0.0935\n",
            "Epoch 00163 | Loss: 0.0989\n",
            "Epoch 00164 | Loss: 0.0988\n",
            "Epoch 00165 | Loss: 0.0925\n",
            "Epoch 00166 | Loss: 0.0896\n",
            "F1-Score: 0.9115\n",
            "Epoch 00167 | Loss: 0.0864\n",
            "Epoch 00168 | Loss: 0.0849\n",
            "Epoch 00169 | Loss: 0.0873\n",
            "Epoch 00170 | Loss: 0.0878\n",
            "Epoch 00171 | Loss: 0.0885\n",
            "F1-Score: 0.9114\n",
            "Epoch 00172 | Loss: 0.0871\n",
            "Epoch 00173 | Loss: 0.0811\n",
            "Epoch 00174 | Loss: 0.0771\n",
            "Epoch 00175 | Loss: 0.0743\n",
            "Epoch 00176 | Loss: 0.0711\n",
            "F1-Score: 0.9201\n",
            "Epoch 00177 | Loss: 0.0717\n",
            "Epoch 00178 | Loss: 0.0704\n",
            "Epoch 00179 | Loss: 0.0714\n",
            "Epoch 00180 | Loss: 0.0672\n",
            "Epoch 00181 | Loss: 0.0653\n",
            "F1-Score: 0.9187\n",
            "Epoch 00182 | Loss: 0.0680\n",
            "Epoch 00183 | Loss: 0.0728\n",
            "Epoch 00184 | Loss: 0.0679\n",
            "Epoch 00185 | Loss: 0.0671\n",
            "Epoch 00186 | Loss: 0.0630\n",
            "F1-Score: 0.9242\n",
            "Epoch 00187 | Loss: 0.0663\n",
            "Epoch 00188 | Loss: 0.0674\n",
            "Epoch 00189 | Loss: 0.0651\n",
            "Epoch 00190 | Loss: 0.0646\n",
            "Epoch 00191 | Loss: 0.0646\n",
            "F1-Score: 0.9219\n",
            "Epoch 00192 | Loss: 0.0695\n",
            "Epoch 00193 | Loss: 0.0703\n",
            "Epoch 00194 | Loss: 0.0733\n",
            "Epoch 00195 | Loss: 0.0744\n",
            "Epoch 00196 | Loss: 0.0678\n",
            "F1-Score: 0.9232\n",
            "Epoch 00197 | Loss: 0.0635\n",
            "Epoch 00198 | Loss: 0.0607\n",
            "Epoch 00199 | Loss: 0.0582\n",
            "Epoch 00200 | Loss: 0.0547\n",
            "Epoch 00201 | Loss: 0.0520\n",
            "F1-Score: 0.9323\n",
            "Epoch 00202 | Loss: 0.0486\n",
            "Epoch 00203 | Loss: 0.0479\n",
            "Epoch 00204 | Loss: 0.0460\n",
            "Epoch 00205 | Loss: 0.0460\n",
            "Epoch 00206 | Loss: 0.0501\n",
            "F1-Score: 0.9320\n",
            "Epoch 00207 | Loss: 0.0549\n",
            "Epoch 00208 | Loss: 0.0538\n",
            "Epoch 00209 | Loss: 0.0551\n",
            "Epoch 00210 | Loss: 0.0551\n",
            "Epoch 00211 | Loss: 0.0549\n",
            "F1-Score: 0.9269\n",
            "Epoch 00212 | Loss: 0.0571\n",
            "Epoch 00213 | Loss: 0.0555\n",
            "Epoch 00214 | Loss: 0.0559\n",
            "Epoch 00215 | Loss: 0.0558\n",
            "Epoch 00216 | Loss: 0.0554\n",
            "F1-Score: 0.9288\n",
            "Epoch 00217 | Loss: 0.0538\n",
            "Epoch 00218 | Loss: 0.0509\n",
            "Epoch 00219 | Loss: 0.0485\n",
            "Epoch 00220 | Loss: 0.0482\n",
            "Epoch 00221 | Loss: 0.0461\n",
            "F1-Score: 0.9348\n",
            "Epoch 00222 | Loss: 0.0467\n",
            "Epoch 00223 | Loss: 0.0477\n",
            "Epoch 00224 | Loss: 0.0483\n",
            "Epoch 00225 | Loss: 0.0598\n",
            "Epoch 00226 | Loss: 0.0628\n",
            "F1-Score: 0.9268\n",
            "Epoch 00227 | Loss: 0.0666\n",
            "Epoch 00228 | Loss: 0.0629\n",
            "Epoch 00229 | Loss: 0.0620\n",
            "Epoch 00230 | Loss: 0.0637\n",
            "Epoch 00231 | Loss: 0.0670\n",
            "F1-Score: 0.9219\n",
            "Epoch 00232 | Loss: 0.0697\n",
            "Epoch 00233 | Loss: 0.0884\n",
            "Epoch 00234 | Loss: 0.0983\n",
            "Epoch 00235 | Loss: 0.1033\n",
            "Epoch 00236 | Loss: 0.0933\n",
            "F1-Score: 0.9103\n",
            "Epoch 00237 | Loss: 0.0871\n",
            "Epoch 00238 | Loss: 0.0818\n",
            "Epoch 00239 | Loss: 0.0691\n",
            "Epoch 00240 | Loss: 0.0605\n",
            "Epoch 00241 | Loss: 0.0532\n",
            "F1-Score: 0.9348\n",
            "Epoch 00242 | Loss: 0.0497\n",
            "Epoch 00243 | Loss: 0.0511\n",
            "Epoch 00244 | Loss: 0.0549\n",
            "Epoch 00245 | Loss: 0.0538\n",
            "Epoch 00246 | Loss: 0.0592\n",
            "F1-Score: 0.9265\n",
            "Epoch 00247 | Loss: 0.0578\n",
            "Epoch 00248 | Loss: 0.0542\n",
            "Epoch 00249 | Loss: 0.0493\n",
            "Epoch 00250 | Loss: 0.0462\n",
            "Epoch 00251 | Loss: 0.0423\n",
            "F1-Score: 0.9411\n",
            "Epoch 00252 | Loss: 0.0394\n",
            "Epoch 00253 | Loss: 0.0400\n",
            "Epoch 00254 | Loss: 0.0413\n",
            "Epoch 00255 | Loss: 0.0405\n",
            "Epoch 00256 | Loss: 0.0370\n",
            "F1-Score: 0.9412\n",
            "Epoch 00257 | Loss: 0.0385\n",
            "Epoch 00258 | Loss: 0.0371\n",
            "Epoch 00259 | Loss: 0.0361\n",
            "Epoch 00260 | Loss: 0.0351\n",
            "Epoch 00261 | Loss: 0.0355\n",
            "F1-Score: 0.9405\n",
            "Epoch 00262 | Loss: 0.0355\n",
            "Epoch 00263 | Loss: 0.0351\n",
            "Epoch 00264 | Loss: 0.0351\n",
            "Epoch 00265 | Loss: 0.0346\n",
            "Epoch 00266 | Loss: 0.0334\n",
            "F1-Score: 0.9416\n",
            "Epoch 00267 | Loss: 0.0351\n",
            "Epoch 00268 | Loss: 0.0348\n",
            "Epoch 00269 | Loss: 0.0353\n",
            "Epoch 00270 | Loss: 0.0333\n",
            "Epoch 00271 | Loss: 0.0368\n",
            "F1-Score: 0.9379\n",
            "Epoch 00272 | Loss: 0.0382\n",
            "Epoch 00273 | Loss: 0.0388\n",
            "Epoch 00274 | Loss: 0.0375\n",
            "Epoch 00275 | Loss: 0.0389\n",
            "Epoch 00276 | Loss: 0.0372\n",
            "F1-Score: 0.9428\n",
            "Epoch 00277 | Loss: 0.0360\n",
            "Epoch 00278 | Loss: 0.0365\n",
            "Epoch 00279 | Loss: 0.0382\n",
            "Epoch 00280 | Loss: 0.0377\n",
            "Epoch 00281 | Loss: 0.0378\n",
            "F1-Score: 0.9386\n",
            "Epoch 00282 | Loss: 0.0386\n",
            "Epoch 00283 | Loss: 0.0384\n",
            "Epoch 00284 | Loss: 0.0386\n",
            "Epoch 00285 | Loss: 0.0393\n",
            "Epoch 00286 | Loss: 0.0403\n",
            "F1-Score: 0.9392\n",
            "Epoch 00287 | Loss: 0.0438\n",
            "Epoch 00288 | Loss: 0.0450\n",
            "Epoch 00289 | Loss: 0.0473\n",
            "Epoch 00290 | Loss: 0.0609\n",
            "Epoch 00291 | Loss: 0.0624\n",
            "F1-Score: 0.9320\n",
            "Epoch 00292 | Loss: 0.0592\n",
            "Epoch 00293 | Loss: 0.0593\n",
            "Epoch 00294 | Loss: 0.0602\n",
            "Epoch 00295 | Loss: 0.0576\n",
            "Epoch 00296 | Loss: 0.0567\n",
            "F1-Score: 0.9306\n",
            "Epoch 00297 | Loss: 0.0534\n",
            "Epoch 00298 | Loss: 0.0515\n",
            "Epoch 00299 | Loss: 0.0474\n",
            "Epoch 00300 | Loss: 0.0477\n",
            "GAT Model : F1-Score on the validation set: 0.9395\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from class_model_gnn import MyGAT\n",
        "\n",
        "### DEVICE GPU OR CPU : will select GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"\\nDevice: \", device)\n",
        "\n",
        "### Max number of epochs\n",
        "max_epochs = 300\n",
        "\n",
        "### Model parameters\n",
        "in_dim = train_dataset[0].x.shape[1]\n",
        "out_dim = train_dataset[0].y.shape[1]\n",
        "hidden_dim = 256\n",
        "num_layers = 2 # ATTENTION : 1 layer implies 1 hidden layer (originally : GAT layer + MLP layer)\n",
        "heads = 4\n",
        "\n",
        "\n",
        "### DEFINE THE GAT MODEL\n",
        "gat_model = MyGAT(in_dim, hidden_dim, out_dim, num_inter_layers=num_layers, num_heads=heads).to(device)\n",
        "\n",
        "### DEFINE LOSS FUNCTION\n",
        "loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "### DEFINE OPTIMIZER\n",
        "optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.005)\n",
        "\n",
        "### TRAIN THE MODEL\n",
        "epoch_list, gat_model_scores = train(\n",
        "    gat_model,\n",
        "    loss_fcn,\n",
        "    device,\n",
        "    optimizer,\n",
        "    max_epochs,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        ")\n",
        "\n",
        "### Save the model\n",
        "torch.save(gat_model.state_dict(), \"gat_model.pth\")\n",
        "\n",
        "### F1-SCORE ON VALID DATASET\n",
        "score_valid = evaluate(gat_model, loss_fcn, device, val_dataloader)\n",
        "print(\"GAT Model : F1-Score on the validation set: {:.4f}\".format(score_valid))\n",
        "\n",
        "\n",
        "### PLOT EVOLUTION OF F1-SCORE W.R.T EPOCHS\n",
        "def plot_f1_score(epoch_list, scores):\n",
        "    plt.figure(figsize=[10, 5])\n",
        "    plt.plot(epoch_list, scores)\n",
        "    plt.title(\"Evolution of F1-Score w.r.t epochs\")\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_f1_score(epoch_list, gat_model_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GAT Model loaded successfully\n",
            "GAT Model : F1-Score on the test set: 0.9591\n",
            "GAT Model : F1-Score on the validation set: 0.9395\n"
          ]
        }
      ],
      "source": [
        "from class_model_gnn import MyGAT\n",
        "### Model parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "in_dim = train_dataset[0].x.shape[1]\n",
        "out_dim = train_dataset[0].y.shape[1]\n",
        "hidden_dim = 256\n",
        "num_layers = 2 # ATTENTION : 1 layer implies 1 hidden layer (originally : GAT layer + MLP layer)\n",
        "heads = 4\n",
        "\n",
        "### DEFINE LOSS FUNCTION\n",
        "loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Load gat_model and do F1-score on test dataset\n",
        "\n",
        "gat_model = MyGAT(in_dim, hidden_dim, out_dim, num_inter_layers=num_layers, num_heads=heads).to(device)\n",
        "gat_model.load_state_dict(torch.load(\"gat_model.pth\", weights_only=True))\n",
        "gat_model.eval()\n",
        "print(\"GAT Model loaded successfully\")\n",
        "\n",
        "score_test = evaluate(gat_model, loss_fcn, device, test_dataloader) \n",
        "print(\"GAT Model : F1-Score on the test set: {:.4f}\".format(score_test))\n",
        "\n",
        "score_valid = evaluate(gat_model, loss_fcn, device, val_dataloader)\n",
        "print(\"GAT Model : F1-Score on the validation set: {:.4f}\".format(score_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "-QUkCjI0eJCQ"
      },
      "source": [
        "## Conv 2D as Message Passing Neural Network\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The use of graph is a way to structure data by adding neighborhood information between features. This then allows to do operations on the data that are local to each node and its neighbors. This is the main idea behind Graph Neural Networks (GNNs). [`pytorch-geometric`](https://pytorch-geometric.readthedocs.io/en/latest/) is a library compatible with PyTorch that allows to easily implement GNNs. The most general structure is the [`MessagePassing`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html#torch_geometric.nn.conv.MessagePassing) class that is then used as a base for more specific GNNs as seen in the course ([Graph Convolutional Networks](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv) or [Graph AttenTion Convolution](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html#torch_geometric.nn.conv.GATConv)).\n",
        "\n",
        "On the other hand, you already know an operation that uses the structure of the data to do local operations: the convolution (https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). One can see the convolution as a specific case of the message passing neural network. The goal of this notebook is to show how to use the `MessagePassing` class to implement a convolutional neural network.\n",
        "You will be asked to implement 3 functions. You should give back those three functions in a file named `conv_as_message_passing.py`. These functions will then be automatically tested. So be sure to respect the function signature and the function name.\n",
        "\n",
        "\n",
        "## Assumptions\n",
        "\n",
        "To make the implementation easier we will make some assumptions:\n",
        "- the input is a single image (batch size of 1) of size 'C x H x W'\n",
        "- the convolution will be a 3x3 kernel with stride 1 and padding 1.\n",
        "\n",
        "You may also assume that the Conv2D layer has no bias but it will be slightly penalized in the grading.\n",
        "\n",
        "Bonus points will be given if you can handle the cases that are not covered by those assumptions.\n",
        "\n",
        "\n",
        "## Questions\n",
        "\n",
        "### Question 2\n",
        "\n",
        "> Using the formalism used in the [`MessagePassing`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html#torch_geometric.nn.conv.MessagePassing) documentation (and on [wikipedia](https://en.wikipedia.org/wiki/Graph_neural_network#Message_passing_layers) with sligthly different notations), explain how theorically you can simulate a 2D convolution using the `MessagePassing` formalism. This may include a pre-processing step to transform the image into a graph and then a post-processing step to transform the graph back into an image. (:warning: Those steps should be independent of the parameters of the convolution, but not necessarily from the hyper-parameters.)\n",
        "$$\\mathbf{x}_{i}^{\\prime} = \\gamma_{\\mathbf{\\Theta}}\\left( \\mathbf{x}_{i},\\bigoplus\\limits_{j \\in \\mathcal{N}(i)}\\,\\phi_{\\mathbf{\\Theta}}\\left( \\mathbf{x}_{i},\\mathbf{x}_{j},\\mathbf{e}_{j,i} \\right) \\right),$$\n",
        "\n",
        "\n",
        "HINT : It is possible to do it with the following $\\gamma$ :\n",
        "\n",
        "$$ \\gamma_\\Theta : x,y \\mapsto y $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af3o_iVWeJCQ"
      },
      "source": [
        "**Answer Q2 :**\n",
        "\n",
        "**Let us recall that : In a 2D convolutional layer, the output feature map $x' \\in \\mathbb{R}^{C' \\times H' \\times W'}$ is computed as:**\n",
        "\n",
        "\\begin{equation}\\tag{eq1}\n",
        "    x'_{i,j,c'} = \\sigma \\left( \\sum_{c=0}^{C-1} \\sum_{m=0}^{K_h-1} \\sum_{n=0}^{K_w-1} \\mathcal{W}_{m,n,c,c'} \\cdot x_{i \\cdot s_h + m - \\lfloor \\frac{K_h}{2} \\rfloor, j \\cdot s_w + n - \\lfloor \\frac{K_w}{2} \\rfloor, c} + b_{c'} \\right)\n",
        "\\end{equation}\n",
        "\n",
        "**where:**\n",
        "\n",
        "- **$x \\in \\mathbb{R}^{C \\times H \\times W}$ is the input.**\n",
        "- **$x' \\in \\mathbb{R}^{C' \\times H' \\times W'}$ is the output feature map.**\n",
        "- **$\\mathcal{W} \\in \\mathbb{R}^{C' \\times C \\times K_h \\times K_w}$ is the convolutional kernel.**\n",
        "- **$b \\in \\mathbb{R}^{C'}$ is the bias.**\n",
        "- **$K_h, K_w$ are the height and width of the kernel.**\n",
        "- **$C$ and $C'$ are the number of input and output channels.**\n",
        "- **$s_h, s_w$ are the stride values for height and width.**\n",
        "- **$p_h, p_w$ are the padding values for height and width.**\n",
        "- **$\\sigma$ is an activation function.**\n",
        "\n",
        "**We easily observe that for each $I=(i,j)$, we create a features map points $x'_{I}$ solely based on the neigbhoring pixels (neigbhors of a distance linked to the kernel sizes). Thus, we will define a graph which has pixels $x_I'$ as nodes and edges between pixels that are useful for output computation (where $I'$ is computed based on $I$ and the hyper-parameters).**\n",
        "\n",
        "**IMPORTANT - We will reduce the assumptions made before into only those assumptions (they are definitely weaker):**\n",
        "- **$K_w$ and $K_h$ are odds integers (not only $3\\times3$), we need that to easily define a clear point on which we apply the kernel.**\n",
        "- **$p_h \\leq \\lfloor \\frac{K_h}{2} \\rfloor$ and $p_w \\leq \\lfloor \\frac{K_w}{2} \\rfloor$ (this is not a strong assumptions because a zero-padding with higher values would pretty much be useless).**\n",
        "\n",
        "**We will define an oriented graph where the nodes are the pixels (the features being the RGB channels) and the neighbors in the graph are pixels used in convolution.**\n",
        "\n",
        "**We define the graph $G=(V,E,A)$ where $V=\\{pixel_{ij}\\;|\\; (i,j)\\in \\{0,\\cdots,H-1\\}\\times\\{0,\\cdots,W-1\\}\\}$ and $A$ (the Adjacency matrix) is defined as follows :**\n",
        "\n",
        "$\\forall (i,j), \\,(i',j')\\in \\{0,\\cdots,H-1\\}\\times\\{0,\\cdots,W-1\\},$\n",
        "\n",
        "\\begin{align}\\tag{eq2} A_{(i,j)(i',j')} = \\begin{cases} 1 + (i'-i+\\lfloor \\frac{K_h}{2} \\rfloor)K_w+j'-j+\\lfloor \\frac{K_w}{2} \\rfloor, & \\text{if } (i,j)\\mathcal{R}_{conv}(i',j')  \\\\ 0, & \\text{ otherwise } \\end{cases} \\end{align}\n",
        "\n",
        "**Where we define the relation $\\mathcal{R}_{conv}$ as follows :**\n",
        "\n",
        "**$(i,j)$ and $(i',j')$ verify $(i,j)\\mathcal{R}_{conv}(i',j')$ if and only if (LowBorder), (UpBorder), (Stride) and (KernelNeigbhor) hold together :**\n",
        "\\begin{align}\n",
        "i \\geq \\lfloor \\frac{K_h}{2} \\rfloor -p_h \\text{ and } j \\geq \\lfloor \\frac{K_w}{2} \\rfloor -p_w \\tag{LowBorder} \\\\\n",
        "i < H + p_h -\\lfloor \\frac{K_h}{2} \\rfloor \\text{ and } j < W + p_w -\\lfloor \\frac{K_w}{2} \\rfloor \\tag{UpBorder} \\\\\n",
        "i + \\lfloor \\frac{K_h}{2} \\rfloor -p_h \\equiv 0 \\pmod{s_h} \\text{ and } j + \\lfloor \\frac{K_w}{2} \\rfloor -p_w \\equiv 0 \\pmod{s_w} \\tag{Stride} \\\\\n",
        "|i-i'|\\leq \\lfloor \\frac{K_h}{2} \\rfloor \\text{ and } |j-j'|\\leq \\lfloor \\frac{K_w}{2} \\rfloor \\tag{KernelNeigbhor}\n",
        "\\end{align}\n",
        "\n",
        "**From this we easily define $E$ : $((i,j),(i',j'))\\in E$ if and only if $A_{(i,j)(i',j')} > 0$ (note that the graph is oriented).**\n",
        "\n",
        "**Thus, we can rewrite the convolutional layer as follows : $\\forall (I,J)\\in \\{0,\\cdots,H'-1\\}\\times\\{0,\\cdots,W'-1\\}$ we define the pixel equivalent as $(i,j) = (I \\cdot s_h - p_h + \\left\\lfloor \\frac{K_h}{2} \\right\\rfloor, J \\cdot s_w - p_w + \\left\\lfloor \\frac{K_w}{2} \\right\\rfloor)$**\n",
        "\n",
        "**We introduce $ \\gamma_\\Theta : (x,y) \\mapsto \\sigma(y + b) $ and**\n",
        "$$\\phi_\\Theta : (x_{(i_1,j_1)}, x_{(i_2,j_2)}, A_{(i_1,j_1)(i_2,j_2)}) \\mapsto \\left( \\sum_{c=0}^{C-1} \\mathcal{W}_{m,n,c,c'} \\cdot x_{(i_2, j_2), c} \\right)_{c'\\in \\{0,\\cdots, C'-1\\}}$$\n",
        "**Where $m = \\left\\lfloor \\frac{A_{(i_1,j_1)(i_2,j_2)}-1}{K_w} \\right\\rfloor$ and $n = A_{(i_1,j_1)(i_2,j_2)} -1 - m \\cdot K_w$**\n",
        "\n",
        "**Therefore, we have :**\n",
        "$$\\mathbf{x}_{(I,J)}^{\\prime} = \\gamma_{\\mathbf{\\Theta}}\\left( \\mathbf{x}_{(i,j)},\\sum\\limits_{(i',j') \\in \\mathcal{N}((i,j))}\\,\\phi_{\\mathbf{\\Theta}}\\left( \\mathbf{x}_{(i,j)},\\mathbf{x}_{(i',j')},\\mathbf{A}_{(i,j)(i',j')} \\right) \\right),$$\n",
        "\n",
        "**Recall that $(i,j)$ is defined based on $(I,J)$.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "2_RtlXxUeJCR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "OR946Cr7eJCR"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "> Implement the pre-processing function, you can use the follwing code skeleton (you may change the output type, it is just a strong suggestion):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "587BgnAQeJCR"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Batch\n",
        "\n",
        "def relation_conv(H, W, Kh, Kw, ph, pw, sh, sw, i, j, ni, nj):\n",
        "    \"\"\"\n",
        "    Computes the relation convolution between two pixels.\n",
        "\n",
        "    Arguments:\n",
        "    ----------\n",
        "    Kh : int\n",
        "        Height of the kernel.\n",
        "    Kw : int\n",
        "        Width of the kernel.\n",
        "    ph : int\n",
        "        Padding height.\n",
        "    pw : int\n",
        "        Padding width.\n",
        "    sh : int\n",
        "        Stride height.\n",
        "    sw : int\n",
        "        Stride width.\n",
        "    i : int\n",
        "        Row index of the first pixel.\n",
        "    j : int\n",
        "        Column index of the first pixel.\n",
        "    ni : int\n",
        "        Row index of the second pixel.\n",
        "    nj : int\n",
        "        Column index of the second pixel.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    bool\n",
        "        Return True if the two verify Rconv defined in Question 2.\n",
        "    \"\"\"\n",
        "    low_border = (i >= Kh // 2 - ph) and (j >= Kw // 2 - pw)\n",
        "    up_border = (i < H + ph - Kh // 2) and (j < W + pw - Kw // 2)\n",
        "    stride = ((i + Kh // 2 - ph) % sh == 0) and ((j + Kw // 2 - pw) % sw == 0)\n",
        "    kernel_neighbor = (abs(i - ni) <= Kh // 2) and (abs(j - nj) <= Kw // 2) # Should always be True with the current definition of ni and nj\n",
        "\n",
        "    return low_border and up_border and stride and kernel_neighbor\n",
        "\n",
        "def image_to_graph(\n",
        "    image: torch.Tensor, conv2d: torch.nn.Conv2d | None = None, flow: str = \"source_to_target\"\n",
        ") -> torch_geometric.data.Data:\n",
        "    \"\"\"\n",
        "    Converts an image tensor to a PyTorch Geometric Data object.\n",
        "    COMPLETE\n",
        "\n",
        "    Arguments:\n",
        "    ----------\n",
        "    image : torch.Tensor\n",
        "        Image tensor of shape (C, H, W).\n",
        "    conv2d : torch.nn.Conv2d, optional\n",
        "        Conv2d layer to simulate, by default None\n",
        "        Is used to determine the size of the receptive field.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    torch_geometric.data.Data\n",
        "        Graph representation of the image.\n",
        "    \"\"\"\n",
        "    batch = True\n",
        "    Kh, Kw = conv2d.kernel_size[0], conv2d.kernel_size[1]\n",
        "    ph, pw = conv2d.padding[0], conv2d.padding[1]\n",
        "    sh, sw = conv2d.stride[0], conv2d.stride[1]\n",
        "    if conv2d is not None:\n",
        "        assert Kh % 2 == 1 and Kw % 2 == 1, \"Expected odd kernel sizes.\"\n",
        "        assert ph <= Kh // 2 and pw <= Kw // 2, \"Padding should be less than or equal to half of the kernel size, no need for more.\"\n",
        "    if len(image.shape) == 3:\n",
        "        image = image.unsqueeze(0)\n",
        "        batch = False\n",
        "    N, C, H, W = image.shape\n",
        "    R_conv = lambda i, j, ni, nj: relation_conv(H, W, Kh, Kw, ph, pw, sh, sw, i, j, ni, nj)\n",
        "\n",
        "    graphs = []\n",
        "\n",
        "    for n in range(N):\n",
        "        nodes = []\n",
        "        edges = []\n",
        "        edge_attrs = []\n",
        "\n",
        "        for i in range(H):\n",
        "            for j in range(W):\n",
        "                node_index = i * W + j\n",
        "                nodes.append(image[n, :, i, j])\n",
        "\n",
        "                for di in range(-Kh // 2, Kh // 2 + 1):\n",
        "                    for dj in range(-Kw // 2, Kw // 2 + 1):\n",
        "                        ni, nj = i + di, j + dj\n",
        "                        verify = ni >= 0 and ni < H and nj >= 0 and nj < W\n",
        "                        if R_conv(i, j, ni, nj) :\n",
        "                            if verify:\n",
        "                                neighbor_index = ni * W + nj\n",
        "                                edge_value = 1 + (ni - i + Kh // 2) * Kw + nj - j + Kw // 2 # 1 + (i'-i+\\lfloor \\frac{K_h}{2} \\rfloor)K_w+j'-j+\\lfloor \\frac{K_w}{2} \\rfloor\n",
        "                                if flow == \"source_to_target\":\n",
        "                                    edges.append([neighbor_index, node_index])\n",
        "                                elif flow == \"target_to_source\":\n",
        "                                    edges.append([node_index, neighbor_index])\n",
        "                                else:\n",
        "                                    raise ValueError(\"flow should be 'source_to_target' or 'target_to_source'\")\n",
        "                                edge_attrs.append(edge_value)\n",
        "        image_size = torch.zeros(C)\n",
        "        if C > 1:\n",
        "            image_size[0] = H\n",
        "            image_size[1] = W\n",
        "        else:\n",
        "            image_size[0] = H\n",
        "        nodes.append(image_size)\n",
        "        nodes = torch.stack(nodes)\n",
        "        edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        edge_attrs = torch.tensor(edge_attrs, dtype=torch.long).unsqueeze(1)\n",
        "\n",
        "        data = torch_geometric.data.Data(x=nodes, edge_index=edges, edge_attr=edge_attrs)\n",
        "        graphs.append(data)\n",
        "    if batch:\n",
        "        print(\"Image to Graph - Batch image mode with {} images\".format(N))\n",
        "        batch_graphs = Batch.from_data_list(graphs)\n",
        "    else:\n",
        "        print(\"Image to Graph - Single image mode\")\n",
        "        batch_graphs = graphs[0]\n",
        "\n",
        "    return batch_graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "V1ECpNMOeJCS"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "> Implement the post-processing function, you can use the follwing code skeleton:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "PvADw-eTeJCS"
      },
      "outputs": [],
      "source": [
        "def graph_to_image(\n",
        "    data, height: int | None = None, width: int | None = None, conv2d: torch.nn.Conv2d | None = None, verbose: bool = True\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Converts a graph representation of an image to an image tensor.\n",
        "\n",
        "    Arguments:\n",
        "    ----------\n",
        "    data : torch.Tensor\n",
        "        Graph data representation of the image.\n",
        "    height : int\n",
        "        Height of the image.\n",
        "    width : int\n",
        "        Width of the image.\n",
        "    conv2d : torch.nn.Conv2d, optional\n",
        "        Conv2d layer to simulate, by default None\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    torch.Tensor\n",
        "        Image tensor of shape (C, H, W).\n",
        "    \"\"\"\n",
        "    if conv2d is None:\n",
        "        conv2d = torch.nn.Conv2d(C, C, kernel_size=3, padding=1, stride=1)\n",
        "    Kh, Kw = conv2d.kernel_size[0], conv2d.kernel_size[1]\n",
        "    ph, pw = conv2d.padding[0], conv2d.padding[1]\n",
        "    sh, sw = conv2d.stride[0], conv2d.stride[1]\n",
        "    if conv2d is not None:\n",
        "        assert Kh % 2 == 1 and Kw % 2 == 1, \"Expected odd kernel sizes.\"\n",
        "        assert ph <= Kh // 2 and pw <= Kw // 2, \"Padding should be less than or equal to half of the kernel size, no need for more.\"\n",
        "\n",
        "    if isinstance(data, torch_geometric.data.Batch):\n",
        "        N = data.num_graphs\n",
        "        if verbose:\n",
        "            print(\"Generating a batch of images with batch_size = \", N)\n",
        "        C, H, W = data.x.shape[1], height, width\n",
        "        list_of_graphs = data.to_data_list()\n",
        "        if H is None or W is None:\n",
        "            if C > 1:\n",
        "                H, W = int(list_of_graphs[0].x[-1][0].item()), int(list_of_graphs[0].x[-1][1].item())\n",
        "            else:\n",
        "                H = int(list_of_graphs[0].x[-1].item())\n",
        "                W = data.num_nodes // N // H\n",
        "        images = torch.zeros((N, C, H, W))\n",
        "\n",
        "        for n in range(N):\n",
        "            graph = list_of_graphs[n]\n",
        "            if H*W != graph.x.shape[0] - 1:\n",
        "                raise ValueError(\"It seems that the batch of images does not have the same sizes.\")\n",
        "            for i in range(H):\n",
        "                for j in range(W):\n",
        "                    node_index = i * W + j\n",
        "                    images[n, :, i, j] = graph.x[node_index]\n",
        "    elif isinstance(data, torch_geometric.data.Data):\n",
        "        if verbose:\n",
        "            print(\"Generating a single image\")\n",
        "        C, H, W = data.x.shape[1], height, width\n",
        "        if H is None or W is None:\n",
        "            if C > 1:\n",
        "                H, W = int(data.x[-1][0].item()), int(data.x[-1][1].item())\n",
        "            else:\n",
        "                H = int(data.x[-1].item())\n",
        "                W = data.num_nodes // H\n",
        "        images = torch.zeros((C, H, W))\n",
        "\n",
        "        for i in range(H):\n",
        "            for j in range(W):\n",
        "                node_index = i * W + j\n",
        "                images[:, i, j] = data.x[node_index]\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"Assuming Data.x has been provided -> No graph batch or single graph detected.\")\n",
        "        C, H, W = data.shape[1], height, width\n",
        "        if H is None or W is None:\n",
        "            if C > 1:\n",
        "                H, W = int(data[-1][0].item()), int(data[-1][1].item())\n",
        "            else:\n",
        "                raise ValueError(\"Height and Width should be provided to tell wether a Tensor is a batch or a single image.\")\n",
        "        N = data.shape[0] // (H * W)\n",
        "        if N != 1:\n",
        "            if verbose:\n",
        "                print(\"Assuming a batch of images has been provided with batch_size = \", N)\n",
        "            images = torch.zeros((N, C, H, W))\n",
        "            for n in range(N):\n",
        "                for i in range(H):\n",
        "                    for j in range(W):\n",
        "                        node_index = i * W + j\n",
        "                        images[n, :, i, j] = data[node_index + n * (H * W) + n]\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"Generating a single image\")\n",
        "            C, H, W = data.shape[1], height, width\n",
        "            if H is None or W is None:\n",
        "                H, W = int(data[-1][0].item()), int(data[-1][1].item())\n",
        "            images = torch.zeros((C, H, W))\n",
        "\n",
        "            for i in range(H):\n",
        "                for j in range(W):\n",
        "                    node_index = i * W + j\n",
        "                    images[:, i, j] = data[node_index]\n",
        "\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "r8y9goSTeJCS"
      },
      "source": [
        "#### Recommended test cases\n",
        "\n",
        "We **encourage** you to test that you have the property that the pre-processing function followed by the post-processing function is the identity function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "ivMNlB83eJCS",
        "outputId": "166e4552-2c72-4b58-86a4-fd5b8294dcf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image to Graph - Batch image mode with 1 images\n",
            "Graph data object:  DataBatch(x=[73, 2], edge_index=[2, 858], edge_attr=[858, 1], batch=[73], ptr=[2])\n",
            "Graph data object x:  torch.Size([73, 2])\n",
            "Assuming Data.x has been provided -> No graph batch or single graph detected.\n",
            "Generating a single image\n",
            "\n",
            "   Conversion functions are correct.\n"
          ]
        }
      ],
      "source": [
        "ref_conv = torch.nn.Conv2d(5, 7, kernel_size=7, padding=1, stride=1)\n",
        "image = torch.randn(1, 2, 8, 9)\n",
        "g_image = image_to_graph(image, ref_conv)\n",
        "print(\"Graph data object: \", g_image)\n",
        "print(\"Graph data object x: \", g_image.x.shape)\n",
        "reconstructed_image = graph_to_image(g_image.x, 8, 9, ref_conv)\n",
        "assert torch.allclose(image, reconstructed_image)\n",
        "print(\"\\n   Conversion functions are correct.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "ak27fLYqeJCT"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "> Implement the `Conv2dMessagePassing` class that will simulate a 2D convolution using the `MessagePassing` formalism.\n",
        "You should inherit from the `MessagePassing` class and only change the `__init__` and `message` functions (the `forward` function has already been changed for you). You should use the following code skeleton:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "JiPQmCfOeJCT"
      },
      "outputs": [],
      "source": [
        "def compute_output_size(H, W, Kh, Kw, ph, pw, sh, sw):\n",
        "    H_prime = (H + 2 * ph - Kh) // sh + 1\n",
        "    W_prime = (W + 2 * pw - Kw) // sw + 1\n",
        "    return H_prime, W_prime\n",
        "\n",
        "def compute_output_index(i, j, Kh, Kw, ph, pw, sh, sw):\n",
        "    i_prime = (i - Kh // 2 + ph) // sh\n",
        "    j_prime = (j - Kw // 2 + pw) // sw\n",
        "    return i_prime, j_prime\n",
        "\n",
        "class Conv2dMessagePassing(torch_geometric.nn.MessagePassing):\n",
        "    \"\"\"\n",
        "    A Message Passing layer that simulates a given Conv2d layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, conv2d: torch.nn.Conv2d, flow=\"source_to_target\", implemented=False, direct=False):\n",
        "        super().__init__(aggr='sum')  # \"Add\" aggregation.\n",
        "        self.Kh, self.Kw = conv2d.kernel_size\n",
        "        self.ph, self.pw = conv2d.padding\n",
        "        self.sh, self.sw = conv2d.stride\n",
        "        self.in_channels = conv2d.in_channels\n",
        "        self.out_channels = conv2d.out_channels\n",
        "        self.weight = conv2d.weight # Shape: (out_channels, in_channels, Kh, Kw)\n",
        "        self.bias = conv2d.bias # Shape: (out_channels)\n",
        "        self.conv2d = conv2d\n",
        "        self.H = None\n",
        "        self.W = None\n",
        "        self.H_prime = None\n",
        "        self.W_prime = None\n",
        "        self.edge_index = None\n",
        "        self.num_graphs = None\n",
        "        self.flow = flow\n",
        "        self.implemented = implemented # If True, the propagation method is implemented, otherwise it is the basic propagate function.\n",
        "        self.direct = direct # If True, the output is computed directly with formula given in Q2.\n",
        "        self.compute_out_size = lambda H, W: compute_output_size(H, W, self.Kh, self.Kw, self.ph, self.pw, self.sh, self.sw)\n",
        "        self.compute_out_index = lambda i, j: compute_output_index(i, j, self.Kh, self.Kw, self.ph, self.pw, self.sh, self.sw)\n",
        "\n",
        "    def forward(self, data):\n",
        "        if isinstance(data, torch_geometric.data.Batch):\n",
        "            self.num_graphs = data.num_graphs\n",
        "            list_of_graphs = data.to_data_list()\n",
        "        else:\n",
        "            self.num_graphs = 1\n",
        "            list_of_graphs = [data]\n",
        "\n",
        "        outputs = []\n",
        "        for graph in list_of_graphs:\n",
        "            x, edge_index, edge_attr = graph.x, graph.edge_index, graph.edge_attr\n",
        "            self.edge_index = edge_index\n",
        "            if self.in_channels > 1:\n",
        "                H, W = int(x[-1][0].item()), int(x[-1][1].item())\n",
        "            else:\n",
        "                H = int(x[-1].item())\n",
        "                W = graph.num_nodes // self.num_graphs // H\n",
        "            self.H, self.W = H, W\n",
        "            x = x[:-1]\n",
        "            self.H_prime, self.W_prime = self.compute_out_size(H, W)\n",
        "            if self.implemented and not self.direct:\n",
        "                out = self.propagation(edge_index, x=x, edge_attr=edge_attr, flow=self.flow)\n",
        "                out = self.transform_output(out)\n",
        "            elif not self.implemented and not self.direct:\n",
        "                out = self.propagate(edge_index, x=x, edge_attr=edge_attr, flow=self.flow)\n",
        "                out = self.transform_output(out)\n",
        "            else:\n",
        "                out = self.compute_out_directly(x, edge_attr)\n",
        "            outputs.append(out)\n",
        "        if self.direct :\n",
        "            if self.num_graphs == 1:\n",
        "                return outputs[0]\n",
        "            return torch.stack(outputs).view(self.num_graphs, outputs[0].shape[0], outputs[0].shape[1], outputs[0].shape[2]) # (num_graphs, out_channels, H', W')\n",
        "        else :\n",
        "            return torch.stack(outputs).view(self.num_graphs * outputs[0].shape[0], outputs[0].shape[1])\n",
        "\n",
        "    def message(self, x_j: torch.Tensor, x_i: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes the message to be passed for each edge.\n",
        "        For each edge e = (u, v) in the graph indexed by i,\n",
        "        the message trough the edge e (ie from node u to node v)\n",
        "        should be returned as the i-th line of the output tensor.\n",
        "        (The message is phi(u, v, e) in the formalism.)\n",
        "        To do this you can access the features of the source node\n",
        "        in x_j[i] and the attributes of the edge in edge_attr[i].\n",
        "\n",
        "        Arguments:\n",
        "        ----------\n",
        "        x_j : torch.Tensor\n",
        "            The features of the source node for each edge (of size E x in_channels).\n",
        "        edge_attr : torch.Tensor\n",
        "            The attributes of the edge (of size E x edge_attr_dim).\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        torch.Tensor\n",
        "            The message to be passed for each edge (of size COMPLETE)\n",
        "        \"\"\"\n",
        "        if self.bias is not None:\n",
        "            if self.flow == \"source_to_target\":\n",
        "                node_counts = torch.bincount(self.edge_index[1, :], minlength=self.edge_index.max().item() + 1)\n",
        "                node_counts = node_counts[self.edge_index[1, :]]\n",
        "            elif self.flow == \"target_to_source\":\n",
        "                node_counts = torch.bincount(self.edge_index[0, :], minlength=self.edge_index.max().item() + 1)\n",
        "                node_counts = node_counts[self.edge_index[0, :]]\n",
        "        C_prime, nb_edges = self.out_channels, x_j.shape[0]\n",
        "        phi = torch.zeros((nb_edges, C_prime))\n",
        "        for idx in range(nb_edges):\n",
        "            edge_value = int(edge_attr[idx]) - 1\n",
        "            m, n = edge_value // self.Kw, edge_value % self.Kw\n",
        "            for c_prime in range(C_prime):\n",
        "                for c in range(self.in_channels):\n",
        "                    phi[idx, c_prime] += x_j[idx, c] * self.weight[c_prime, c, m, n]\n",
        "                if self.bias is not None:\n",
        "                    phi[idx, c_prime] += self.bias[c_prime] * (1/node_counts[idx])\n",
        "        return phi\n",
        "\n",
        "    def transform_output(self, out):\n",
        "        \"\"\"\n",
        "        Transforms the output of the message passing to the expected shape -> Indeed, we get (num_nodes, out_channels) as shape.\n",
        "        But for convolution many of those nodes are useless to us. We want to get the output in the shape (H'*W', out_channels).\n",
        "        \"\"\"\n",
        "        output = torch.zeros((self.H_prime * self.W_prime + 1, self.out_channels))\n",
        "        if self.flow == \"source_to_target\":\n",
        "            unique_nodes = torch.unique(self.edge_index[1, :])\n",
        "        elif self.flow == \"target_to_source\":\n",
        "            unique_nodes = torch.unique(self.edge_index[0, :])\n",
        "        for idx in unique_nodes:\n",
        "            i_prime, j_prime = self.compute_out_index(idx // self.W, idx % self.W)\n",
        "            output[i_prime * self.W_prime + j_prime, :] = out[idx]\n",
        "        if self.out_channels > 1:\n",
        "            output[-1, :2] = torch.tensor([self.H_prime, self.W_prime])\n",
        "        else:\n",
        "            output[-1, :1] = torch.tensor([self.H_prime])\n",
        "        return output\n",
        "    \n",
        "    def propagation(self, edge_index, x=None, edge_attr=None, flow=\"source_to_target\"):\n",
        "        \"\"\"\n",
        "        My own implemented propagate function : propagates messages along the edges of the graph.\n",
        "        You can use it by changing the preset parameter 'implemented' to True.\n",
        "        \"\"\"\n",
        "        if flow == \"source_to_target\":\n",
        "            x_j = x[edge_index[0]]\n",
        "            x_i = x[edge_index[1]]\n",
        "        elif flow == \"target_to_source\":\n",
        "            x_j = x[edge_index[1]]\n",
        "            x_i = x[edge_index[0]]\n",
        "        else:\n",
        "            raise ValueError(\"Flow should be either 'source_to_target' or 'target_to_source'.\")\n",
        "        message = self.message(x_j, x_i, edge_attr)\n",
        "        assert message.shape == (edge_index.shape[1], self.out_channels)\n",
        "        out = torch.zeros((x.shape[0], self.out_channels))\n",
        "        for idx in range(edge_index.shape[1]):\n",
        "            i, j = int(edge_index[0, idx]), int(edge_index[1, idx])\n",
        "            if flow == \"source_to_target\":\n",
        "                out[j] += message[idx]\n",
        "            elif flow == \"target_to_source\":\n",
        "                out[i] += message[idx]\n",
        "        return out\n",
        "    \n",
        "    def compute_out_directly(self, x: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes the output of the layer directly without message passing.\n",
        "        \"\"\"\n",
        "        C_prime, H_prime, W_prime = self.out_channels, self.H_prime, self.W_prime\n",
        "        Final = torch.zeros((C_prime, H_prime, W_prime))\n",
        "        for idx in range(self.edge_index.shape[1]):\n",
        "            if self.flow == \"source_to_target\":\n",
        "                i, j = int(self.edge_index[1, idx]), int(self.edge_index[0, idx])\n",
        "            elif self.flow == \"target_to_source\":\n",
        "                i, j = int(self.edge_index[0, idx]), int(self.edge_index[1, idx])\n",
        "            i_prime, j_prime = self.compute_out_index(i // self.W, i % self.W)\n",
        "            edge_value = int(edge_attr[idx]) - 1\n",
        "            m, n = edge_value // self.Kw, edge_value % self.Kw\n",
        "            for c_prime in range(C_prime):\n",
        "                for c in range(self.in_channels):\n",
        "                    Final[c_prime, i_prime, j_prime] += x[j, c] * self.weight[c_prime, c, m, n]\n",
        "                if self.bias is not None:\n",
        "                    Final[c_prime, i_prime, j_prime] += self.bias[c_prime]\n",
        "\n",
        "        return Final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "BBQUc7M2eJCT"
      },
      "source": [
        "## Test example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "cell_ktag": "BFQQXMXLtuY4",
        "id": "CtWjI19beJCT",
        "outputId": "48d0d9b7-e6d2-47a6-885d-f0bb5af08b6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image to Graph - Single image mode\n",
            "Graph data object:  Data(x=[301, 1], edge_index=[2, 3072], edge_attr=[3072, 1])\n",
            "\n",
            "  FLOW:  target_to_source\n",
            "\n",
            "   Message Passing Convolution is correct.\n",
            "\n",
            "   My Implemented Message Passing Convolution is correct.\n"
          ]
        }
      ],
      "source": [
        "N = 1\n",
        "c = 1\n",
        "h = 20\n",
        "w = 15\n",
        "\n",
        "kernel_size = 7\n",
        "padding = 3\n",
        "stride = 2\n",
        "c_prime = 5\n",
        "\n",
        "bias = True\n",
        "\n",
        "# You can choose the flow of the graph as you prefer (source_to_taget implies that the target node is the center on which the kernel is applied)\n",
        "flow = \"source_to_target\"\n",
        "flow = \"target_to_source\"\n",
        "\n",
        "ref_conv = torch.nn.Conv2d(c, c_prime, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)\n",
        "image = torch.randn(c, h, w)\n",
        "\n",
        "g_image = image_to_graph(image, ref_conv, flow=flow)\n",
        "print(\"Graph data object: \", g_image)\n",
        "ref_conv.weight.data = torch.randn_like(ref_conv.weight.data)\n",
        "\n",
        "y_th = ref_conv(image)\n",
        "\n",
        "conv_mp = Conv2dMessagePassing(ref_conv, implemented=False, flow=flow) # Original Message Passing method from pytorch geometric (propagate function)\n",
        "graphic_image_1 = conv_mp(g_image)\n",
        "\n",
        "conv_mp = Conv2dMessagePassing(ref_conv, implemented=True, flow=flow) # My Message Passing method (propagation function implemented)\n",
        "graphic_image_2 = conv_mp(g_image)\n",
        "\n",
        "h_prime, w_prime = compute_output_size(h, w, kernel_size, kernel_size, padding, padding, stride, stride) # To reconstruct the image in case the kernel changed the size of the output.\n",
        "\n",
        "reconstructed_image_1 = graph_to_image(graphic_image_1, h_prime, w_prime, ref_conv, verbose=False)\n",
        "reconstructed_image_2 = graph_to_image(graphic_image_2, h_prime, w_prime, ref_conv, verbose=False)\n",
        "\n",
        "print(\"\\n  FLOW: \", flow)\n",
        "assert torch.allclose(y_th, reconstructed_image_1, atol=1e-4)\n",
        "print(\"\\n   Message Passing Convolution is correct.\")\n",
        "\n",
        "assert torch.allclose(y_th, reconstructed_image_2, atol=1e-4)\n",
        "print(\"\\n   My Implemented Message Passing Convolution is correct.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### All the test together without having to run the rest of the notebook :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bernas/.pyenv/versions/3.11.0/envs/MVA/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------Test Image to Graph to Image--------------------------------------------------\n",
            "Image to Graph - Batch image mode with 1 images\n",
            "Graph data object:  DataBatch(x=[73, 2], edge_index=[2, 858], edge_attr=[858, 1], batch=[73], ptr=[2])\n",
            "Graph data object x:  torch.Size([73, 2])\n",
            "Assuming Data.x has been provided -> No graph batch or single graph detected.\n",
            "Generating a single image\n",
            "\n",
            "   Conversion functions are correct.\n",
            "--------------------------------------------------Test Message Passing Convolution--------------------------------------------------\n",
            "Image to Graph - Single image mode\n",
            "Graph data object:  Data(x=[226, 1], edge_index=[2, 2304], edge_attr=[2304, 1])\n",
            "\n",
            "  FLOW:  target_to_source\n",
            "\n",
            "   Message Passing Convolution is correct.\n",
            "\n",
            "   My Implemented Message Passing Convolution is correct.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Batch\n",
        "from conv_as_message_passing import image_to_graph, graph_to_image, compute_output_size, Conv2dMessagePassing\n",
        "\n",
        "##################################################################################################################################\n",
        "print(\"-\"*50 + \"Test Image to Graph to Image\" + \"-\"*50)\n",
        "\n",
        "ref_conv = torch.nn.Conv2d(5, 7, kernel_size=7, padding=1, stride=1)\n",
        "image = torch.randn(1, 2, 8, 9)\n",
        "g_image = image_to_graph(image, ref_conv)\n",
        "print(\"Graph data object: \", g_image)\n",
        "print(\"Graph data object x: \", g_image.x.shape)\n",
        "reconstructed_image = graph_to_image(g_image.x, 8, 9, ref_conv)\n",
        "assert torch.allclose(image, reconstructed_image)\n",
        "print(\"\\n   Conversion functions are correct.\")\n",
        "\n",
        "##################################################################################################################################\n",
        "print(\"-\"*50 + \"Test Message Passing Convolution\" + \"-\"*50)\n",
        "\n",
        "N = 1\n",
        "c = 1\n",
        "h = 15\n",
        "w = 15\n",
        "\n",
        "kernel_size = 7\n",
        "padding = 3\n",
        "stride = 2\n",
        "c_prime = 5\n",
        "\n",
        "bias = True\n",
        "\n",
        "# You can choose the flow of the graph as you prefer (source_to_taget implies that the target node is the center on which the kernel is applied)\n",
        "flow = \"source_to_target\"\n",
        "flow = \"target_to_source\"\n",
        "\n",
        "ref_conv = torch.nn.Conv2d(c, c_prime, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)\n",
        "image = torch.randn(c, h, w)\n",
        "\n",
        "g_image = image_to_graph(image, ref_conv, flow=flow)\n",
        "print(\"Graph data object: \", g_image)\n",
        "ref_conv.weight.data = torch.randn_like(ref_conv.weight.data)\n",
        "\n",
        "y_th = ref_conv(image)\n",
        "\n",
        "conv_mp = Conv2dMessagePassing(ref_conv, implemented=False, flow=flow) # Original Message Passing method from pytorch geometric (propagate function)\n",
        "graphic_image_1 = conv_mp(g_image)\n",
        "\n",
        "conv_mp = Conv2dMessagePassing(ref_conv, implemented=True, flow=flow) # My Message Passing method (propagation function implemented)\n",
        "graphic_image_2 = conv_mp(g_image)\n",
        "\n",
        "h_prime, w_prime = compute_output_size(h, w, kernel_size, kernel_size, padding, padding, stride, stride) # To reconstruct the image in case the kernel changed the size of the output.\n",
        "\n",
        "reconstructed_image_1 = graph_to_image(graphic_image_1, h_prime, w_prime, ref_conv, verbose=False)\n",
        "reconstructed_image_2 = graph_to_image(graphic_image_2, h_prime, w_prime, ref_conv, verbose=False)\n",
        "\n",
        "print(\"\\n  FLOW: \", flow)\n",
        "assert torch.allclose(y_th, reconstructed_image_1, atol=1e-4)\n",
        "print(\"\\n   Message Passing Convolution is correct.\")\n",
        "\n",
        "assert torch.allclose(y_th, reconstructed_image_2, atol=1e-4)\n",
        "print(\"\\n   My Implemented Message Passing Convolution is correct.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MVA",
      "language": "python",
      "name": "python3"
    },
    "kfiletag": "BFQQXMXLtuY4",
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
